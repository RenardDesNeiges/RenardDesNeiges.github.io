<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>RL with Time Continuous Neural Nets | Titouan Renard</title> <meta name="author" content="Titouan Renard"> <meta name="description" content="Learning motor policies with time continuous neural networks."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/al-folio/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/al-folio/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/al-folio/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A7%A0&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/al-folio/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://alshedivat.github.io/al-folio/projects/2_LTC/"> <link rel="stylesheet" href="/al-folio/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/al-folio/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/al-folio/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/al-folio/"><span class="font-weight-bold">Titouan </span>Renard</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/al-folio/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/al-folio/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/al-folio/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/al-folio/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/al-folio/teaching/">teaching</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">RL with Time Continuous Neural Nets</h1> <p class="post-description">Learning motor policies with time continuous neural networks.</p> </header> <article> <p>Time-Continuous Neural Networks provide an effective framework for the modeling of dynamical systems and are natural candidates for continuous-control tasks. They are closely related to the dynamics of non-spiking neurons, which gives further justification to investigate their use in control. This post gives a summary of the work and results obtained while investigating the use of such neural networks during a semester project that I worked jointly supervised by EPFL’s <a href="https://www.epfl.ch/labs/biorob/" rel="external nofollow noopener" target="_blank">BIOROB</a> and <a href="https://www.epfl.ch/labs/lcn/" rel="external nofollow noopener" target="_blank">LCN</a> labs.</p> <h2 id="time-continuous-neural-networks">Time-continuous neural networks</h2> <p>Time-Continuous Neural Networks model dynamical systems model the evolution of the hidden states (which we denote as \(x_t\) at a given time \(t\)) of a neural network by equations of the form:</p> \[\frac{\partial x_t}{\partial t} = D(xt, It, θ)\] <p>Where D denotes some kind of model function that estimates the time-derivative of \(x_t\). \(D\) is a function of \(x_t\), which denotes the hidden state of the neuron, It which denotes the inputs of the neuron and a learnable parameter vector \(\theta\). Updates of the (hidden) state \(x_t\) are the computed using some ODE solver which integrates the flow \(D(x_t, I_t, \theta)\) over some time-step \(\Delta_t\) to compute:</p> \[x_{t+\Delta_t} = \int^{t+\Delta_t}_t D(x_t, I_t, \theta).\] <p>It is worth noting that in a supervised learning context, this formulation has the advantage of being able to represent irregularly sampled time-sequences. For the control applications we consider here, the sample-rate is imposed by the hardware of our robot and is likely regular.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/al-folio/assets/img/unroll-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/al-folio/assets/img/unroll-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/al-folio/assets/img/unroll-1400.webp"></source> <img src="/al-folio/assets/img/unroll.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Such a model can be though of as a recurrent neural network where the recurrent connection are implemented by an integrator. </div> <p>The most straight forward approach to implementing a continuous-time neural network is to directly use a neural network to model the flow D this approach is often referred to as “<a href="https://arxiv.org/abs/1806.07366" rel="external nofollow noopener" target="_blank">Neural-ODEs</a>”: Where the flow \(D(x_t, I_t, \theta)\) is given by the output of a neural network:</p> \[D(x_t, I_t, \theta) = f(x_t, I_t, θ).\] <p>An alternative provided by an earlier contribution is the so called “<a href="https://www.sciencedirect.com/science/article/abs/pii/S089360800580125X" rel="external nofollow noopener" target="_blank">Continuous-Time Recurrent Neural Network</a>” model (CT-RNNs) introduces a stabilization term to an equivalent formulation:</p> \[D(x,I,\theta)=−\frac{x_t}{\tau} + f(x,I,\theta).\] <p>Finally, a more recent approach referred to as “<a href="https://arxiv.org/abs/2006.04439" rel="external nofollow noopener" target="_blank">Liquid Time Constant Neural Networks</a>” introduces further non-linearity by having \(f\) affect the time-constant (hence make the time-constant “liquid”), this approach corresponds to the following time-derivative model:</p> \[D(x,I,\theta)= - \Bigg[\frac{1}{\tau} + A f(x,I,\theta) \Bigg]x_t + f(x,I,\theta).\] <p>Compared to multi-layer perceptrons and RNNs computing gradients on continuous time neural networks is less obvious because of the integrator step introduced in the computation of the inner state x. There are two different possible approach to the computation of such gradients together with their pros and cons: backpropagation through time and the adjoint sensitivity method.</p> <p>Backpropagation through time (<code class="language-plaintext highlighter-rouge">BPTT</code>) works by directly computed the gradient of a loss function through the ODE solver (it requires our ODE solver to build a computation graph and then we use autograd to compute a gradient). <code class="language-plaintext highlighter-rouge">BPTT</code> requires saving the inner state of neurons for every step of the ODE solver, this comes at a high memory cost.</p> <p>The adjoint sensitivity method, required saving a so-called adjoint state \(a(t) = \frac{\partial L}{\partial x_t}\) throughout the unrolling of the neural network. Given some Loss function:</p> \[L(x(t)) = L \Bigg( x(t_0) + \int_{t_0}^{t_1} D(xt, It, θ) dt\Bigg)\] <p>we define our adjoint sensitivity state \(a(t)\). The idea here is to use the stored \(a(t)\) to compute a gradient:</p> \[\frac{\partial L(x(t))}{\partial t} = \int_{t_1}^{t_0} a(t)^T \frac{\partial D(x(t),I_t,\theta)}{\partial \theta} dt\] <p>Note that we compute the loss backwards through time (from \(t_1\) to \(t_0\)) which is why the negative sign appears in front of the integral. Which we can use to perform gradient descent on our model. This computation is performed using the same numerical ODE solver as for the forward pass.</p> <h2 id="building-a-neural-network-cell-for-motor-control">Building a neural network cell for motor control</h2> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/al-folio/assets/img/LTC-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/al-folio/assets/img/LTC-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/al-folio/assets/img/LTC-1400.webp"></source> <img src="/al-folio/assets/img/LTC.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> A visual representation of a fully connected Time-Continuous Neural Network Cell. With 4 inputs, 5 inner-neurons and 2 motor neurons. </div> <p>In order to deal with the motor task that we consider in this project we choose to investigate small fully connected neuron cells that take a \(n\)-dimensional input, contain \(k\) inner-neurons and return \(d\) outputs (we call the outputs “motor neurons”). Most of our experiments are performed with LTC cells, so we will explicitly derive the forward pass equations for an LTC, but we can build equivalent cells for RNN-ODE and CT-RNN cells in a very similar fashion.</p> <h2 id="rl-training-and-experiments">RL training and experiments</h2> <p>In order to train our fully connected time-continuous neural network cell, we implement a batched version of the PPO algorithm, that we modify to work with the adjunct method of gradients computation.<br> The reinforcement learning-based training of a time-continuous neuron cell requires the setup of a learning environment. Such an environment must be able to:</p> <ol> <li>simulate the POMDP (since we investigate a continuous control task, this is the physics of our controlled system),</li> <li>implement the policy model (in our case our time-continuous neuron cells),</li> <li>train it using our RL algorithm (PPO).</li> </ol> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/al-folio/assets/img/Framework-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/al-folio/assets/img/Framework-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/al-folio/assets/img/Framework-1400.webp"></source> <img src="/al-folio/assets/img/Framework.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Reinforcement learning environment architecture. </div> <p>In order to investigate the efficiency of TCNN cells we setup a simple (and rather classical in the RL literature) motor control experiment: the cart-pole problem. We use the Mujoco gym implementation of cart-pole together with our implementation of <code class="language-plaintext highlighter-rouge">PPO</code> training for <code class="language-plaintext highlighter-rouge">TCNN</code> policies to perform our experiments. We use the adjoint method for the computation of gradients and focus our experiments on LTC cells. The code used for the experiments is available at <a href="https://github.com/RenardDesNeiges/CTNN_Policies_DERL" rel="external nofollow noopener" target="_blank">https://github.com/RenardDesNeiges/CTNN_Policies_DERL</a>.</p> <div class="row center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/al-folio/assets/img/inv_pendulum_ltc_1.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/al-folio/assets/img/inv_pendulum_ltc_1.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/al-folio/assets/img/inv_pendulum_ltc_1.gif-1400.webp"></source> <img src="/al-folio/assets/img/inv_pendulum_ltc_1.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> An example trajectory produced by the trained policy. </div> <p>We find that the LTC cell trained with PPO can find efficient solutions to the cart-pole problem, but that it does so with a particularly low sample efficiency (compared to a reference Multi-Layer Perceptron (MLP) model trained with a perceptron). On the other hand the amount of neurons required to achieve a good solution is extremely low compared to the amount of MLP neurons that would be required for solving the problem with a MLP model. This ability of the model to find low-neuron count solutions isn’t of interest for computational reasons (recall the number of parameters per neuron is much higher than in the case of an MLP) but may prove interesting as that low neuron count may make the system easier to interpret.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/al-folio/assets/img/tcnn_convergence-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/al-folio/assets/img/tcnn_convergence-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/al-folio/assets/img/tcnn_convergence-1400.webp"></source> <img src="/al-folio/assets/img/tcnn_convergence.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Learning curves of policies trained on fully-connected cells with different neuron counts. </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Titouan Renard. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/al-folio/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/al-folio/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/al-folio/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/al-folio/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/al-folio/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/al-folio/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>